{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Downloading pip-21.2.2-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.4\n",
      "    Uninstalling pip-20.2.4:\n",
      "      Successfully uninstalled pip-20.2.4\n",
      "Successfully installed pip-21.2.2\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp36-cp36m-manylinux2014_x86_64.whl (37.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python-headless) (1.19.4)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.5.3.56\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torch-1.9.0%2Bcu102-cp36-cp36m-linux_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 14 kB/s  eta 0:00:011     |█████▌                          | 141.5 MB 18.1 MB/s eta 0:00:39     |█████▋                          | 147.0 MB 18.1 MB/s eta 0:00:38     |███████                         | 184.4 MB 17.6 MB/s eta 0:00:37     |████████▏                       | 212.1 MB 15.2 MB/s eta 0:00:41     |█████████▏                      | 237.2 MB 16.7 MB/s eta 0:00:36     |█████████▏                      | 238.1 MB 16.7 MB/s eta 0:00:36     |█████████████▏                  | 341.2 MB 16.5 MB/s eta 0:00:30     |███████████████▋                | 406.5 MB 18.9 MB/s eta 0:00:23     |█████████████████▎              | 449.9 MB 15.9 MB/s eta 0:00:24     |███████████████████████▎        | 603.9 MB 16.5 MB/s eta 0:00:14     |███████████████████████▉        | 619.6 MB 17.9 MB/s eta 0:00:12     |████████████████████████████▌   | 740.6 MB 16.1 MB/s eta 0:00:06     |██████████████████████████████▌ | 792.0 MB 2.8 MB/s eta 0:00:15\n",
      "\u001b[?25hCollecting torchvision==0.10.0+cu102\n",
      "  Downloading https://download.pytorch.org/whl/cu102/torchvision-0.10.0%2Bcu102-cp36-cp36m-linux_x86_64.whl (22.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.1 MB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio===0.9.0\n",
      "  Downloading torchaudio-0.9.0-cp36-cp36m-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.9.0+cu102) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.9.0+cu102) (0.8)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.10.0+cu102) (8.0.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.10.0+cu102) (1.19.4)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.9.0+cu102 torchaudio-0.9.0 torchvision-0.10.0+cu102\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q -q -q tifffile\n",
    "!pip install -q -q -q tqdm\n",
    "!pip install opencv-python-headless\n",
    "!pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (8.0.1)\n",
      "Collecting Pillow\n",
      "  Downloading Pillow-8.3.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.0.1\n",
      "    Uninstalling Pillow-8.0.1:\n",
      "      Successfully uninstalled Pillow-8.0.1\n",
      "Successfully installed Pillow-8.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture des librairies et framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "nom du GPU : Tesla T4\n",
      "GPU initialisé :  True\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import s3fs\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tifffile import TiffFile\n",
    "import matplotlib.pyplot as plt\n",
    "import colorsys\n",
    "import PIL\n",
    "from PIL import Image \n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "from framework.dataset import LandCoverData as LCD\n",
    "\n",
    "device= torch.device(\"cuda:0\" )#if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(\"Using {} device\".format(device))\n",
    "print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "print(\"GPU initialisé : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Téléchargement + lecture des images de la BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n",
    "fs.get('projet-challengedata-ens/data/additional_files_earthcube_emu4zqr.zip', 'additional_files_earthcube_emu4zqr.zip')\n",
    "shutil.unpack_archive('additional_files_earthcube_emu4zqr.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "\n",
    "DATA_FOLDER_STR = 'dataset'\n",
    "DATA_FOLDER = Path(DATA_FOLDER_STR).expanduser()\n",
    "# path to the unzipped dataset: contains directories train/ and test/\n",
    "DATASET_FOLDER = DATA_FOLDER\n",
    "\n",
    "# get all train images and masks\n",
    "train_images_paths = sorted(list(DATASET_FOLDER.glob('train/images/*.tif')))\n",
    "train_masks_paths = sorted(list(DATASET_FOLDER.glob('train/masks/*.tif')))\n",
    "# get all test images\n",
    "test_images_paths = sorted(list(DATASET_FOLDER.glob('test/images/*.tif')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainning and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, train=True):   # initial logic happens like transform\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms.ToTensor()\n",
    "        self.maskcol = np.empty((1,10))\n",
    "    def __getitem__(self, idx):\n",
    "        #tmask = np.empty((0,65536))\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        with TiffFile(self.image_paths[idx]) as tif :\n",
    "            img = tif.asarray()\n",
    "        if img.dtype == np.uint16:\n",
    "            scale = 255/2200\n",
    "            byte_im = (img)*scale\n",
    "            byte_im = (byte_im.clip(0, 255) + 0.5).astype(np.uint8)\n",
    "        image = byte_im\n",
    "        maskcol = self.maskcol\n",
    "        with TiffFile(self.mask_paths[idx]) as tif :\n",
    "            mask = tif.asarray()\n",
    "            maskcol[0][0] = np.count_nonzero(mask == 0)\n",
    "            maskcol[0][1] = np.count_nonzero(mask == 1)\n",
    "            maskcol[0][2] = np.count_nonzero(mask ==2)\n",
    "            maskcol[0][3] = np.count_nonzero(mask == 3)\n",
    "            maskcol[0][4] = np.count_nonzero(mask == 4)\n",
    "            maskcol[0][5] = np.count_nonzero(mask == 5)\n",
    "            maskcol[0][6] = np.count_nonzero(mask == 6)\n",
    "            maskcol[0][7] = np.count_nonzero(mask == 7)\n",
    "            maskcol[0][8] = np.count_nonzero(mask == 8)\n",
    "            maskcol[0][9] = np.count_nonzero(mask == 9)    \n",
    "            maskcol = maskcol/65536\n",
    "        #tmask = np.concatenate((tmask, [mask]), axis = 0)\n",
    "        t_image = self.transforms(image)\n",
    "        t_mask = self.transforms(maskcol)\n",
    "        #t_mask = torch.tensor(mask)#.type(torch.LongTensor)\n",
    "        #print(idx)\n",
    "        return t_image, t_mask\n",
    "\n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([20, 4, 256, 256]) torch.float32\n",
      "Shape of y:  torch.Size([20, 1, 1, 10]) torch.float64\n",
      "shape of b: tensor([[0.0000e+00, 0.0000e+00, 1.3855e+00, 4.9831e+01, 1.4943e+01, 5.3879e+00,\n",
      "         2.5900e+01, 0.0000e+00, 0.0000e+00, 2.5528e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 4.9088e+00, 5.1393e+01, 1.0066e+01, 2.0065e+00,\n",
      "         3.1625e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 8.3893e+00, 5.1115e+01, 5.8334e+00, 1.0788e+00,\n",
      "         3.3574e+01, 0.0000e+00, 0.0000e+00, 9.1553e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 1.2268e+00, 6.4474e+01, 1.4841e+01, 2.8000e+00,\n",
      "         1.6658e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 8.6823e-01, 5.4375e+01, 1.5834e+01, 2.1210e-01,\n",
      "         2.8610e+01, 0.0000e+00, 0.0000e+00, 1.0071e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.1623e+01, 3.3682e+01, 1.9406e+01, 1.8738e+00,\n",
      "         3.3415e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 4.8828e+01, 3.8696e+01, 6.3354e+00, 2.0248e+00,\n",
      "         4.0237e+00, 0.0000e+00, 0.0000e+00, 9.1553e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 1.8823e+01, 2.3203e+01, 2.1228e+01, 6.6437e+00,\n",
      "         2.7078e+01, 2.2049e+00, 0.0000e+00, 8.1940e-01],\n",
      "        [0.0000e+00, 2.0142e-01, 2.6079e+01, 5.6847e+01, 9.1660e+00, 9.7046e-01,\n",
      "         5.5130e+00, 0.0000e+00, 0.0000e+00, 1.2238e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0884e+00, 7.9445e+01, 9.5703e+00,\n",
      "         7.8964e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.1347e+00, 1.4752e+01, 3.1097e+00, 4.1656e-01,\n",
      "         6.3863e+01, 0.0000e+00, 0.0000e+00, 1.5724e+01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0071e-01, 9.1095e-01, 6.2167e+01, 1.1322e+00,\n",
      "         3.5556e+01, 1.3275e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 2.7924e-01, 5.5873e+01, 1.9424e+01, 1.6156e+01,\n",
      "         3.4943e+00, 0.0000e+00, 0.0000e+00, 4.7729e+00],\n",
      "        [0.0000e+00, 1.0376e-01, 2.8244e+00, 3.8336e+01, 5.3238e+00, 4.5929e-01,\n",
      "         5.0327e+01, 5.4932e-02, 0.0000e+00, 2.5711e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 7.6157e+00, 6.2656e+01, 4.8370e+00, 1.6815e+00,\n",
      "         2.3210e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.8903e+01, 3.1232e+01, 3.2736e+01, 2.7878e+00,\n",
      "         1.0112e+01, 0.0000e+00, 0.0000e+00, 4.2297e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.6853e+01, 3.5522e+01, 1.0759e+01, 4.5654e+00,\n",
      "         3.2246e+01, 0.0000e+00, 0.0000e+00, 5.3406e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 3.1143e+00, 3.8754e+01, 2.6691e+01, 1.0040e+00,\n",
      "         2.9225e+01, 0.0000e+00, 0.0000e+00, 1.2115e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 8.9722e-01, 6.3437e+01, 1.2399e+01, 2.0187e+00,\n",
      "         2.1066e+01, 0.0000e+00, 0.0000e+00, 1.8158e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 1.1536e+01, 2.6871e+01, 1.3150e+01, 4.7501e+00,\n",
      "         4.3694e+01, 0.0000e+00, 0.0000e+00, 0.0000e+00]], device='cuda:0') torch.Size([20, 10]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "dataloader  = trainloader\n",
    "size = len(dataloader.dataset)\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "    X, y = X.to(device), y.to(device)\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape, X.dtype)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    b = y.view(y.shape[0],y.shape[3]).float()*100\n",
    "    print(\"shape of b:\", b, b.shape, b.dtype)\n",
    "    #loss = loss_fn(pred, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3 4 ... 6 6 6]]\n",
      "[[6 6 6 ... 6 6 6]]\n",
      "[[6 6 6 ... 6 6 6]]\n",
      "[[3 3 3 ... 3 3 3]]\n",
      "[[3 3 3 ... 6 6 6]]\n",
      "[[4 6 6 ... 6 6 6]]\n",
      "[[3 5 5 ... 6 6 6]]\n",
      "[[6 6 6 ... 4 4 4]]\n",
      "[[3 3 3 ... 3 3 3]]\n",
      "[[3 3 3 ... 3 3 3]]\n",
      "[[5 5 6 ... 3 3 3]]\n",
      "[[6 4 4 ... 3 3 6]]\n",
      "[[3 6 6 ... 6 6 6]]\n",
      "[[5 5 5 ... 3 3 5]]\n",
      "[[6 6 6 ... 5 5 5]]\n",
      "[[6 6 6 ... 4 5 5]]\n",
      "[[4 4 4 ... 3 3 3]]\n",
      "[[4 5 5 ... 5 5 5]]\n",
      "[[4 4 4 ... 4 4 4]]\n",
      "[[3 3 3 ... 2 2 2]]\n",
      "[[4 4 4 ... 6 6 6]]\n",
      "[[3 3 3 ... 6 6 6]]\n",
      "[[3 3 3 ... 6 6 6]]\n",
      "[[4 4 4 ... 2 2 3]]\n",
      "[[6 6 6 ... 4 4 4]]\n",
      "[[7 7 7 ... 9 9 9]]\n",
      "[[3 3 3 ... 6 6 6]]\n",
      "[[3 3 3 ... 3 3 3]]\n",
      "[[6 6 6 ... 6 6 6]]\n",
      "Shape of X [N, C, H, W]:  torch.Size([20, 4, 256, 256])\n",
      "Shape of y:  torch.Size([20, 1, 65536]) torch.int64\n",
      "[[4 4 4 ... 4 4 4]]\n",
      "[[3 3 3 ... 4 4 4]]\n",
      "[[3 3 3 ... 6 6 6]]\n",
      "[[4 4 6 ... 4 4 4]]\n",
      "[[3 3 3 ... 3 3 3]]\n",
      "[[6 4 5 ... 6 6 6]]\n",
      "[[3 3 3 ... 3 3 3]]\n",
      "[[3 3 3 ... 4 4 6]]\n",
      "[[4 4 4 ... 6 6 6]]\n",
      "[[2 2 6 ... 6 6 6]]\n",
      "[[3 3 3 ... 3 3 3]]\n"
     ]
    }
   ],
   "source": [
    "for X, y in trainloader:\n",
    "    #print(y)\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    #print(y)\n",
    "    break\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, batch_size):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y = y.view(y.shape[0],y.shape[3]).float().to(device)\n",
    "        #print(\"shape of y:\", y.shape, y.dtype)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        #print(pred)\n",
    "        #print(\"shape of pred:\", pred.shape, pred.dtype)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "def test(dataloader, model, loss_fn, batch_size):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0 \n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y = y.view(y.shape[0],y.shape[3]).float().to(device)        \n",
    "            pred = model(X)\n",
    "            #a = loss_fn(pred, y)\n",
    "            #test_loss += a\n",
    "            test_loss += loss_fn(pred,y).item()\n",
    "            correct += ((pred - y)**2).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv1 = nn.Conv2d(in_channels= 4, out_channels= 6, kernel_size=3,stride=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels= 6, out_channels= 2, kernel_size=6,stride=3)\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(1458, 700)  # 5*5 from image dimension\n",
    "        self.fc2 = nn.Linear(700, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (self.conv1(x))\n",
    "        x = (self.maxpool(x))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = (F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([4096, 65536, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 5124) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 5124) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1d3d0452861d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get some random training images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# show images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 5124) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images,masks, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(image))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18491\n",
      "12956.0\n",
      "temps en seconde :  3.492848784662783\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split these path using a certain percentage\n",
    "len_data = len(train_images_paths)\n",
    "print(len_data)\n",
    "train_size = 0.7006651884700665\n",
    "print(len_data*train_size)\n",
    "train_image_paths = train_images_paths[:int(len_data*train_size)]\n",
    "test_image_paths = train_images_paths[int(len_data*train_size):]\n",
    "\n",
    "train_mask_paths = train_masks_paths[:int(len_data*train_size)]\n",
    "test_mask_paths = train_masks_paths[int(len_data*train_size):]\n",
    "\n",
    "batch = 20\n",
    "#création du train\n",
    "train_dataset = CustomDataset(train_image_paths, train_mask_paths, train=True)\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "#création du test du train\n",
    "test_dataset = CustomDataset(test_image_paths, test_mask_paths, train=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch, shuffle=False)\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"temps en seconde : \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6360, 0.7237, 0.6196, 0.6543, 0.6652, 0.6639, 0.6048, 0.6124, 0.6456,\n",
      "        0.6394, 0.6630, 0.5972, 0.6507, 0.6363, 0.6395, 0.7070, 0.6353, 0.6477,\n",
      "        0.6423, 0.6091], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch, (X, y) in enumerate(dataloader):\n",
    "        #z = y.reshape(batch_size,65536).to(device)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        print(torch.sum(pred, dim=1))\n",
    "        y = torch.sum(y.view(y.shape[0],y.shape[3]).float(), dim=1)\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.682000  [    0/12956]\n",
      "loss: 2.867550  [ 4000/12956]\n",
      "loss: 2.782336  [ 6000/12956]\n",
      "loss: 2.047265  [ 8000/12956]\n",
      "loss: 2.914593  [10000/12956]\n",
      "loss: 2.581081  [12000/12956]\n",
      "Test Error: \n",
      " Accuracy: 23.5%, Avg loss: 2.637546 \n",
      "\n",
      "Done!\n",
      "temps en seconde :  1026.4305839156732\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "epochs = 1\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainloader, model, loss_fn, optimizer, batch)\n",
    "    pred, y = test(testloader, model, loss_fn, batch)\n",
    "    #print(a.shape)\n",
    "print(\"Done!\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"temps en seconde : \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred :  tensor([[0.0000, 0.0000, 0.0572, 0.3900, 0.0000, 0.0812, 0.2765, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0592, 0.3627, 0.0000, 0.0815, 0.2658, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0548, 0.3724, 0.0000, 0.0783, 0.2589, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0595, 0.3527, 0.0000, 0.0752, 0.2561, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0529, 0.3374, 0.0000, 0.0816, 0.2519, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0546, 0.3452, 0.0000, 0.0784, 0.2547, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0466, 0.3514, 0.0000, 0.0789, 0.2504, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0566, 0.3577, 0.0000, 0.0828, 0.2586, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0564, 0.3723, 0.0000, 0.0869, 0.2634, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0630, 0.3628, 0.0000, 0.0862, 0.2590, 0.0018, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0565, 0.3449, 0.0000, 0.0803, 0.2505, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0565, 0.3514, 0.0000, 0.0853, 0.2557, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0549, 0.3600, 0.0000, 0.0760, 0.2531, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0562, 0.3740, 0.0000, 0.0807, 0.2633, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 0.0573, 0.3814, 0.0000, 0.0803, 0.2688, 0.0000, 0.0000,\n",
      "         0.0000]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-b788a263374d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(\"mask : \", (pred-y)*100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "print(\"pred : \", pred)\n",
    "A = ((pred-y).to('cpu')).numpy()\n",
    "a = torch.sum(pred, dim=1)\n",
    "print(a)\n",
    "#print(\"mask : \", (pred-y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-28b368599edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpred2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#print(a.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-65bf9450fc0e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#print(\"shape of pred:\", pred.shape, pred.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1121\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Long but got scalar type Float for argument #2 'target' in call to _thnn_nll_loss_forward"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "epochs = 5\n",
    "loss_fn2 = nn.CrossEntropyLoss()\n",
    "optimizer2 = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(trainloader, model, loss_fn2, optimizer2, batch)\n",
    "    pred2, y2 = test(testloader, model, loss_fn2, batch)\n",
    "    #print(a.shape)\n",
    "print(\"Done!\")\n",
    "\n",
    "end = time.perf_counter()\n",
    "print(\"temps en seconde : \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 6, 254, 254])\n",
      "torch.Size([20, 6, 84, 84])\n",
      "torch.Size([20, 2, 27, 27])\n",
      "1458\n",
      "torch.Size([20, 2, 27, 27])\n"
     ]
    }
   ],
   "source": [
    "# Pour avoir un élément du dataloader (ie un batch)\n",
    "X,y  = next(iter(trainloader)) \n",
    "convolution = nn.Conv2d(in_channels= 4, out_channels= 6, kernel_size=3,stride=1)\n",
    "m = nn.MaxPool2d(3, stride=3)\n",
    "convolution2 = nn.Conv2d(in_channels= 6, out_channels= 2, kernel_size=6,stride=3)\n",
    "\n",
    "X_2 = convolution(X)\n",
    "M = m(X_2)\n",
    "X_3 = convolution2(M)\n",
    "print(X_2.shape)\n",
    "print(M.shape)\n",
    "print(X_3.shape)\n",
    "print(X_3.shape[1]*X_3.shape[2]*X_3.shape[3])\n",
    "print(F.relu(X_3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0078, 0.0078, 0.0078,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         [0.0275, 0.0275, 0.0275,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         [0.0078, 0.0275, 0.0235,  ..., 0.0353, 0.0353, 0.0353],\n",
      "         ...,\n",
      "         [0.0157, 0.0157, 0.0157,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0235, 0.0235, 0.0235]],\n",
      "\n",
      "        [[0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0118, 0.0118],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0118, 0.0118],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0118, 0.0118],\n",
      "         ...,\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235]],\n",
      "\n",
      "        [[0.0196, 0.0196, 0.0196,  ..., 0.0078, 0.0078, 0.0118],\n",
      "         [0.0196, 0.0196, 0.0196,  ..., 0.0078, 0.0078, 0.0118],\n",
      "         [0.0196, 0.0196, 0.0196,  ..., 0.0078, 0.0078, 0.0118],\n",
      "         ...,\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0235, 0.0235, 0.0157],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0235, 0.0235, 0.0157],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0118, 0.0157]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         ...,\n",
      "         [0.0196, 0.0196, 0.0196,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0196, 0.0196, 0.0196,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0196, 0.0196, 0.0196,  ..., 0.0157, 0.0157, 0.0157]],\n",
      "\n",
      "        [[0.0118, 0.0118, 0.0118,  ..., 0.0157, 0.0157, 0.0157],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0118, 0.0235],\n",
      "         [0.0118, 0.0118, 0.0118,  ..., 0.0118, 0.0118, 0.0235],\n",
      "         ...,\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235]],\n",
      "\n",
      "        [[0.0235, 0.0235, 0.0235,  ..., 0.0118, 0.0118, 0.0118],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0118, 0.0118, 0.0118],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0118, 0.0118, 0.0118],\n",
      "         ...,\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235],\n",
      "         [0.0235, 0.0235, 0.0235,  ..., 0.0235, 0.0235, 0.0235]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 256, 256])\n",
      "torch.Size([41, 4, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dataloader = trainloader\n",
    "for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        #z = y.reshape(batch_size,65536).to(device)\n",
    "        print(y[:,0])\n",
    "        print(y[3].shape)\n",
    "        print(X.shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 6 6 ... 6 6 6]\n",
      " [6 6 6 ... 6 6 6]\n",
      " [6 6 6 ... 6 6 6]\n",
      " ...\n",
      " [3 3 3 ... 6 6 6]\n",
      " [3 3 3 ... 6 6 6]\n",
      " [3 3 3 ... 6 6 6]]\n",
      "\n",
      "tensor([[6, 6, 6,  ..., 6, 6, 6],\n",
      "        [6, 6, 6,  ..., 6, 6, 6],\n",
      "        [6, 6, 6,  ..., 6, 6, 6],\n",
      "        ...,\n",
      "        [3, 3, 3,  ..., 6, 6, 6],\n",
      "        [3, 3, 3,  ..., 6, 6, 6],\n",
      "        [3, 3, 3,  ..., 6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "for k, idx in enumerate(random.sample(range(len(train_images_paths)), 1)):\n",
    "    image_path = train_images_paths[idx]\n",
    "    mask_path = train_masks_paths[idx]\n",
    "    assert image_path.name == mask_path.name\n",
    "    \n",
    "    with TiffFile(image_path) as tif:\n",
    "        arr = tif.asarray()        \n",
    "    with TiffFile(mask_path) as tif:\n",
    "        mask = tif.asarray()\n",
    "maskt = torch.tensor(mask).type(torch.LongTensor)\n",
    "\n",
    "print(mask)\n",
    "print()\n",
    "print(maskt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12956%41"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
